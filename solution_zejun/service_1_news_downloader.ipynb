{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d7580d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "from typing import Dict, List, Optional\n",
    "import redis\n",
    "import json\n",
    "import uuid\n",
    "import sys\n",
    "from tinydb import TinyDB, Query\n",
    "from datetime import datetime as dt, timedelta\n",
    "import os\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import importlib\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc90104",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"/Users/zejunzheng/Documents/GenAI/stock_searcher\"  # Absolute path\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e671b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis_q.redisUtils as rq # import RQJobQ\n",
    "import dbutils.db_classes as dbs\n",
    "import webUtils.newsUtils as news_cls\n",
    "import config.conf as configure\n",
    "import LLMUtils.llmTools as llm_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f0bf15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_html_folder = configure.paths['raw_html_dir']\n",
    "charDB_path = configure.paths['chart_db']\n",
    "newsDB_path = configure.paths['news_db']\n",
    "jobDB_path = configure.paths['job_db']\n",
    "news_downloader = news_cls.TickerCrawler(raw_html_folder, charDB_path, newsDB_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec09e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_queue = rq.RQJobQ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06684dba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        job_id, ticker = job_queue.get_crawler_job()\n",
    "        \n",
    "        if job_id:\n",
    "            print(f\"Processing job {job_id} - {ticker}\")\n",
    "            status, url_keys, org = await news_downloader.find_or_download_news_urls(ticker, job_id)\n",
    "            print(f\"Completed job {job_id} - {status}\")\n",
    "            print(url_keys)\n",
    "            \n",
    "            print(\"push job to next step -summarisor\")\n",
    "            job_queue.push_summary_job(job_id, url_keys, ticker, org)\n",
    "            print(\"sumary jobs: \", job_queue.RQ.llen(job_queue.sumary_queue_name))\n",
    "      \n",
    "    except redis.exceptions.TimeoutError:\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing job: {e}\")\n",
    "        time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df9e5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sumary jobs:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"sumary jobs: \", job_queue.RQ.llen(job_queue.crawl_queue_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92f27f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
